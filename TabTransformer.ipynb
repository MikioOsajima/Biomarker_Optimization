{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer  # 本体をインポート，．\n",
    "\n",
    "# 連続値の平均と標準偏差を用意（任意）．各連続変数ごとに (mean, std) の行列．\n",
    "cont_mean_std = torch.randn(10, 2)  # shape = (num_continuous, 2)．\n",
    "\n",
    "# モデル定義．カテゴリ数や連続変数数はデータに合わせて変更．\n",
    "model = TabTransformer(\n",
    "    categories=(10, 5, 6, 5, 8),      # 各カテゴリ変数のユニーク値数，．\n",
    "    num_continuous=10,                # 連続変数の数，．\n",
    "    dim=32,                           # 埋め込み次元 (論文は32)，．\n",
    "    dim_out=1,                        # 出力次元 (回帰なら1, 分類ならクラス数)，．\n",
    "    depth=6,                          # Transformer ブロックの深さ (論文推奨6)，．\n",
    "    heads=8,                          # Multi-head attention のヘッド数 (論文推奨8)，．\n",
    "    attn_dropout=0.1,                 # Attention 後のドロップアウト率，．\n",
    "    ff_dropout=0.1,                   # FeedForward 後のドロップアウト率，．\n",
    "    mlp_hidden_mults=(4, 2),          # 最終 MLP の隠れ層サイズ比，．\n",
    "    mlp_act=nn.ReLU(),                # MLP の活性化関数，．\n",
    "    continuous_mean_std=cont_mean_std # (任意) 連続値を正規化するための mean/std，．\n",
    ")  # これでモデル準備完了:contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# ダミーデータで順伝播\n",
    "x_categ = torch.randint(0, 5, (1, 5))  # バッチサイズ1、カテゴリ数5 のインデックスデータ．\n",
    "x_cont  = torch.randn(1, 10)           # バッチサイズ1、連続変数10次元のデータ．\n",
    "\n",
    "pred = model(x_categ, x_cont)         # 出力は shape=(1, dim_out)．\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
