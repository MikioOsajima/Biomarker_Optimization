{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d410fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fold1 モデル読み込み: ./save_mask_models/best_model_epoch300_fold1_ValAcc0.732.pth ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold1 推論:   0%|          | 0/14000 [00:00<?, ?件/s]/tmp/ipykernel_1213669/2490003468.py:89: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  x_cat = torch.tensor([X_test.iloc[i][categorical_features].values], dtype=torch.long).to(DEVICE)\n",
      "Fold1 推論: 100%|██████████| 14000/14000 [29:41<00:00,  7.86件/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold1 - base accuracy: 0.6943, improved accuracy: 0.7056\n",
      "✅ fold1_results.csv saved\n",
      "=== Fold2 モデル読み込み: ./save_mask_models/best_model_epoch300_fold2_ValAcc0.739.pth ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold2 推論: 100%|██████████| 14000/14000 [30:55<00:00,  7.55件/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold2 - base accuracy: 0.6929, improved accuracy: 0.7050\n",
      "✅ fold2_results.csv saved\n",
      "=== Fold3 モデル読み込み: ./save_mask_models/best_model_epoch300_fold3_ValAcc0.731.pth ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold3 推論: 100%|██████████| 14000/14000 [32:27<00:00,  7.19件/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold3 - base accuracy: 0.6954, improved accuracy: 0.7054\n",
      "✅ fold3_results.csv saved\n",
      "=== Fold4 モデル読み込み: ./save_mask_models/best_model_epoch300_fold4_ValAcc0.737.pth ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold4 推論: 100%|██████████| 14000/14000 [31:05<00:00,  7.50件/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold4 - base accuracy: 0.6964, improved accuracy: 0.7013\n",
      "✅ fold4_results.csv saved\n",
      "=== Fold5 モデル読み込み: ./save_mask_models/best_model_epoch300_fold5_ValAcc0.735.pth ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold5 推論: 100%|██████████| 14000/14000 [31:50<00:00,  7.33件/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold5 - base accuracy: 0.6899, improved accuracy: 0.7025\n",
      "✅ fold5_results.csv saved\n"
     ]
    }
   ],
   "source": [
    "#エントロピーを利用した特徴選択とFTTransformerによる心疾患予測\n",
    "#このコードは、心疾患予測のためのFTTransformerモデルを使用し\n",
    "#エントロピーを利用して特徴選択を行うものです。\n",
    "#モデルは5つのfoldで学習され、各foldごとに推論を行い、\n",
    "#改善された予測結果をCSVファイルに保存します。\n",
    "#5に加えて，CSVに各項目の期待値を出力するように改良\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "from fttransformer_mask import FTTransformer\n",
    "\n",
    "# ============ 設定 ============\n",
    "DATA_PATH = \"cardio_train.csv\"\n",
    "TEST_RATIO = 0.2\n",
    "categorical_features = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\"]\n",
    "continuous_features = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\"]\n",
    "additional_features = [\"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\"]\n",
    "MASK_BASE = [False, False] + [True] * len(additional_features) + [False] * len(continuous_features)\n",
    "DEVICE = torch.device(\"cpu\")  # GPU利用時は \"cuda\" に変更\n",
    "\n",
    "# エントロピー計算関数\n",
    "def entropy(p: float) -> float:\n",
    "    if p <= 0 or p >= 1:\n",
    "        return 0.0\n",
    "    return - (p * log(p) + (1 - p) * log(1 - p))\n",
    "\n",
    "# モデル保存パス\n",
    "fold_to_model_path = {\n",
    "    1: \"./save_mask_models/best_model_epoch300_fold1_ValAcc0.732.pth\",\n",
    "    2: \"./save_mask_models/best_model_epoch300_fold2_ValAcc0.739.pth\",\n",
    "    3: \"./save_mask_models/best_model_epoch300_fold3_ValAcc0.731.pth\",\n",
    "    4: \"./save_mask_models/best_model_epoch300_fold4_ValAcc0.737.pth\",\n",
    "    5: \"./save_mask_models/best_model_epoch300_fold5_ValAcc0.735.pth\",\n",
    "}\n",
    "\n",
    "# ============ メイン処理 ============\n",
    "def main():\n",
    "    # データ読み込み＆分割\n",
    "    data = pd.read_csv(DATA_PATH, sep=';')\n",
    "    ids = data[\"id\"].values\n",
    "    X = data.drop(columns=[\"cardio\"])\n",
    "    y = data[\"cardio\"]\n",
    "    X_trainval, X_test, y_trainval, y_test, id_trainval, id_test = train_test_split(\n",
    "        X, y, ids, test_size=TEST_RATIO, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # カテゴリ変数エンコード\n",
    "    encoders = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X_trainval[col] = le.fit_transform(X_trainval[col])\n",
    "        X_test[col] = le.transform(X_test[col])\n",
    "        encoders[col] = le\n",
    "\n",
    "    # 連続値スケーリング\n",
    "    scaler = StandardScaler()\n",
    "    X_trainval[continuous_features] = scaler.fit_transform(X_trainval[continuous_features])\n",
    "    X_test[continuous_features] = scaler.transform(X_test[continuous_features])\n",
    "\n",
    "    # 出現確率再計算\n",
    "    feature_value_probs = {\n",
    "        feat: X_trainval[feat].value_counts(normalize=True).sort_index().to_dict()\n",
    "        for feat in additional_features\n",
    "    }\n",
    "\n",
    "    # 各foldごとに処理\n",
    "    for fold, model_path in fold_to_model_path.items():\n",
    "        print(f\"=== Fold{fold} モデル読み込み: {model_path} ===\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"❌ モデルが存在しません: {model_path}\")\n",
    "            continue\n",
    "\n",
    "        # モデルロード\n",
    "        model = FTTransformer(\n",
    "            categories=[X[cat].nunique() for cat in categorical_features],\n",
    "            num_continuous=len(continuous_features),\n",
    "            dim=64, depth=6, heads=8, ff_dropout=0.2, attn_dropout=0.2\n",
    "        ).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        model.eval()\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # 推論ループ（進捗バー付き）\n",
    "        for i in tqdm(range(len(X_test)), desc=f\"Fold{fold} 推論\", unit=\"件\"):\n",
    "            sid = int(id_test[i])\n",
    "            label = int(y_test.iloc[i])\n",
    "            x_cat = torch.tensor([X_test.iloc[i][categorical_features].values], dtype=torch.long).to(DEVICE)\n",
    "            x_cont = torch.tensor([X_test.iloc[i][continuous_features].values.astype(np.float32)], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "            # ベース推論 + エントロピー\n",
    "            with torch.no_grad():\n",
    "                base_conf = torch.sigmoid(\n",
    "                    model(x_cat, x_cont, torch.tensor([MASK_BASE], dtype=torch.bool).to(DEVICE))\n",
    "                ).item()\n",
    "            base_ent = entropy(base_conf)\n",
    "            base_pred = int(base_conf > 0.5)\n",
    "            was_correct_before = (base_pred == label)\n",
    "\n",
    "            # エントロピー削減量で特徴選択と期待エントロピー取得\n",
    "            expected_ent_map = {}\n",
    "            reduction_map = {}\n",
    "            best_reduction = -1.0\n",
    "            best_feat = None\n",
    "            for feat in additional_features:\n",
    "                mask = MASK_BASE.copy()\n",
    "                mask[2 + additional_features.index(feat)] = False\n",
    "                expected_ent = 0.0\n",
    "                for val, prob in feature_value_probs[feat].items():\n",
    "                    x_alt = x_cat.clone()\n",
    "                    x_alt[0, categorical_features.index(feat)] = int(val)\n",
    "                    with torch.no_grad():\n",
    "                        conf_alt = torch.sigmoid(\n",
    "                            model(x_alt, x_cont, torch.tensor([mask], dtype=torch.bool).to(DEVICE))\n",
    "                        ).item()\n",
    "                    expected_ent += entropy(conf_alt) * prob\n",
    "                expected_ent_map[feat] = expected_ent\n",
    "                reduction_map[feat] = base_ent - expected_ent\n",
    "                if reduction_map[feat] > best_reduction:\n",
    "                    best_reduction = reduction_map[feat]\n",
    "                    best_feat = feat\n",
    "\n",
    "            # 改良後推論\n",
    "            final_mask = MASK_BASE.copy()\n",
    "            final_mask[2 + additional_features.index(best_feat)] = False\n",
    "            with torch.no_grad():\n",
    "                improved_conf = torch.sigmoid(\n",
    "                    model(x_cat, x_cont, torch.tensor([final_mask], dtype=torch.bool).to(DEVICE))\n",
    "                ).item()\n",
    "            imp_pred = int(improved_conf > 0.5)\n",
    "            was_correct_after = (imp_pred == label)\n",
    "\n",
    "            # 変化タイプ\n",
    "            if not was_correct_before and was_correct_after:\n",
    "                change_type = 'incorrect->correct'\n",
    "            elif was_correct_before and not was_correct_after:\n",
    "                change_type = 'correct->incorrect'\n",
    "            elif was_correct_before and was_correct_after:\n",
    "                change_type = 'correct->correct'\n",
    "            else:\n",
    "                change_type = 'incorrect->incorrect'\n",
    "\n",
    "            # ランキング (削減量順)\n",
    "            sorted_feats = sorted(additional_features, key=lambda f: reduction_map[f], reverse=True)\n",
    "            feat_rank_order = \">\".join(sorted_feats)\n",
    "\n",
    "            # 結果記録\n",
    "            row = {\n",
    "                'id': sid,\n",
    "                'answer': label,\n",
    "                'chosen_feat': best_feat,\n",
    "                'base_pred': base_pred,\n",
    "                'improved_pred': imp_pred,\n",
    "                'was_correct_before': was_correct_before,\n",
    "                'was_correct_after': was_correct_after,\n",
    "                'change_type': change_type,\n",
    "                'feat_rank_order': feat_rank_order\n",
    "            }\n",
    "            for feat in additional_features:\n",
    "                row[f'expected_ent_{feat}'] = expected_ent_map[feat]\n",
    "            results.append(row)\n",
    "\n",
    "        # Accuracy, Precision, Recall, F1計算\n",
    "        y_true = [r['answer'] for r in results]\n",
    "        y_base = [r['base_pred'] for r in results]\n",
    "        y_imp = [r['improved_pred'] for r in results]\n",
    "        base_acc = sum(r['was_correct_before'] for r in results) / len(results)\n",
    "        imp_acc = sum(r['was_correct_after'] for r in results) / len(results)\n",
    "        base_prec = precision_score(y_true, y_base)\n",
    "        base_rec = recall_score(y_true, y_base)\n",
    "        base_f1 = f1_score(y_true, y_base)\n",
    "        imp_prec = precision_score(y_true, y_imp)\n",
    "        imp_rec = recall_score(y_true, y_imp)\n",
    "        imp_f1 = f1_score(y_true, y_imp)\n",
    "        print(\n",
    "            f\"Fold{fold} - base acc: {base_acc:.4f}, prec: {base_prec:.4f}, rec: {base_rec:.4f}, f1: {base_f1:.4f} | \"\n",
    "            f\"improved acc: {imp_acc:.4f}, prec: {imp_prec:.4f}, rec: {imp_rec:.4f}, f1: {imp_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 混同行列を横並びでプロット\n",
    "        cm_base = confusion_matrix(y_true, y_base)\n",
    "        cm_imp = confusion_matrix(y_true, y_imp)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        labels = ['nocardio', 'cardio']\n",
    "        ConfusionMatrixDisplay(cm_base, display_labels=labels).plot(ax=axes[0], cmap='Blues', colorbar=False)\n",
    "        axes[0].set_title('Base CM')\n",
    "        ConfusionMatrixDisplay(cm_imp, display_labels=labels).plot(ax=axes[1], cmap='Blues', colorbar=False)\n",
    "        axes[1].set_title('Improved CM')\n",
    "        fig.suptitle(f'Fold{fold} Confusion Matrices')\n",
    "        fig.text(\n",
    "            0.5, -0.05,\n",
    "            f\"Base -> acc: {base_acc:.3f}, prec: {base_prec:.3f}, rec: {base_rec:.3f}, f1: {base_f1:.3f} | \"\n",
    "            f\"Imp -> acc: {imp_acc:.3f}, prec: {imp_prec:.3f}, rec: {imp_rec:.3f}, f1: {imp_f1:.3f}\",\n",
    "            ha='center'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fold{fold}_cms.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # CSV保存\n",
    "        pd.DataFrame(results).sort_values('id').to_csv(f\"fold{fold}_results.csv\", index=False)\n",
    "        print(f\"✅ fold{fold}_results.csv saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
